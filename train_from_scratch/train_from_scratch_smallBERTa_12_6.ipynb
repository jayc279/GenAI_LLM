{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "78bb494e-48fd-4911-b18c-b234819bdf8c",
      "metadata": {
        "id": "78bb494e-48fd-4911-b18c-b234819bdf8c"
      },
      "source": [
        "### Train a small sized ROBERaMLM (84M parameters, 6 layers, 12 attn heads) from scratch\n",
        "\n",
        "To showcase training Huggingface Trassformer model from scratch. Only 10k lines are used from oscar.eo.txt file - original file has approximately 1-million lines.\n",
        "\n",
        "Decreased number of EPOCHs to train model from to `10` - since this more of an exercise to train a Model from scratch, did not shard or execute data parallel.\n",
        "\n",
        "Adding compute_metrics to `training` blows up memory - had to turn-it off on **Colab** since session crashes\n",
        "\n",
        "Notebook uses ByteLevelBPETokenizer from OpenAI to generate tokens for Esperanto"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d19a2c50-4fd6-43bf-b1a8-98aff7201f15",
      "metadata": {
        "id": "d19a2c50-4fd6-43bf-b1a8-98aff7201f15"
      },
      "source": [
        "REF: https://github.com/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fe02d5a-d5ce-434b-a5fb-2e76ddf057d9",
      "metadata": {
        "id": "4fe02d5a-d5ce-434b-a5fb-2e76ddf057d9"
      },
      "source": [
        "### Preface\n",
        "Notebook walks through the various steps on how to train a Transformer model from scratch.\n",
        "The dataset is split into three - train, evaluate, and test. The test set is used for inference and predictions/completions and verfied with ROGUE, BLUE, and BERT scores.\n",
        "\n",
        "There is a lot of literature on tokenization, datasets creation, and data loaders. If you change the model to train, please refer to model documentation on tokenization, and setup steps to train etc...\n",
        "\n",
        "**Note**: could not execute this notebook on local m/c `GeForce GTX 1660 Ti` using complete dataset.  \n",
        "To verify `code` you can split `oscar.eo.txt` file to 1000 lines and train for `1` epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9500c397-5909-4f55-aaa5-f40f293d5b3b",
      "metadata": {
        "id": "9500c397-5909-4f55-aaa5-f40f293d5b3b"
      },
      "outputs": [],
      "source": [
        "PYDEVD_DISABLE_FILE_VALIDATION = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "31c28c00-bcf4-42bd-8156-287f1c1d2625",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31c28c00-bcf4-42bd-8156-287f1c1d2625",
        "outputId": "b3ff2516-7b31-4286-ad68-c6ab7c60dea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.22.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.29.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.19.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "## Requirements\n",
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install kaggle\n",
        "!pip install datasets\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install torchinfo\n",
        "!pip install evaluate\n",
        "# !pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "403d97d9-bd44-4b99-a6f1-6b091e05d52e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "403d97d9-bd44-4b99-a6f1-6b091e05d52e",
        "outputId": "cd0e4344-31ec-4d48-c8f4-b7efe5758b91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bdb4b088-ae07-4e63-9933-d1fd7e830c62",
      "metadata": {
        "id": "bdb4b088-ae07-4e63-9933-d1fd7e830c62"
      },
      "outputs": [],
      "source": [
        "## mount Google Drive is using Google Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "800ec1cc-21d2-4d8a-8af7-263789c75765",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "800ec1cc-21d2-4d8a-8af7-263789c75765",
        "outputId": "96f72670-9d9e-4151-b149-1098cbe7d2f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data folder:/content/datasets/RobertaMLM/oscar_eo\n",
            "model folder:/content/models/RobertaMLM/oscar_eo\n",
            "tokenize folder:/content/tokenizer/RobertaMLM/oscar_eo\n"
          ]
        }
      ],
      "source": [
        "## Set the path to the data folder, datafile and output folder and files\n",
        "root_folder = '/content'\n",
        "model_name = 'RobertaMLM'\n",
        "dataname = 'oscar_eo'\n",
        "if os.path.exists('/content/drive/My Drive/'):\n",
        "  root_folder = '/content/drive/My Drive/'\n",
        "\n",
        "data_folder = os.path.abspath(os.path.join(root_folder, 'datasets', model_name, dataname))\n",
        "model_folder = os.path.abspath(os.path.join(root_folder, 'models', model_name, dataname))\n",
        "tokenizer_dir = os.path.abspath(os.path.join(root_folder, 'tokenizer',  model_name, dataname))\n",
        "\n",
        "print(f'data folder:{data_folder}')\n",
        "print(f'model folder:{model_folder}')\n",
        "print(f'tokenize folder:{tokenizer_dir}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qqzj2s5MB6Dl",
      "metadata": {
        "id": "Qqzj2s5MB6Dl"
      },
      "source": [
        "create directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "-oKUSVeCBywC",
      "metadata": {
        "id": "-oKUSVeCBywC"
      },
      "outputs": [],
      "source": [
        "os.makedirs(data_folder, exist_ok=True)\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "os.makedirs(tokenizer_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e47d7ed-6e69-4b89-93eb-7f8d7d2eacc3",
      "metadata": {
        "id": "2e47d7ed-6e69-4b89-93eb-7f8d7d2eacc3"
      },
      "source": [
        "### Dataset\n",
        "The Notebook is more focussed on training a language model from scratch and less on Exploratory Data Analysis techniques and Data gathering techniques.\n",
        "\n",
        "`oscar.eo.txt` is a one million line corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "11fd0bfe-2dd7-44f9-92a1-7d8eeeeaae48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11fd0bfe-2dd7-44f9-92a1-7d8eeeeaae48",
        "outputId": "35205324-1d1b-408e-c72a-f4acd8d7184a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-21 00:51:41--  https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\n",
            "Resolving cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)... 13.226.225.93, 13.226.225.98, 13.226.225.32, ...\n",
            "Connecting to cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)|13.226.225.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Fetch dataset\n",
        "## in this Notebook, we will use a text file containing Esperanto sentences\n",
        "!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UinxLZh8D0Jk",
      "metadata": {
        "id": "UinxLZh8D0Jk"
      },
      "source": [
        "Use device CUDA - **Compute Unified Device Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1LWj-I2hDyMl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LWj-I2hDyMl",
        "outputId": "fd1ccc14-4e81-4ab6-abf7-51a782f371bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0 True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device, torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e7c21a9-05b7-4dc6-818c-e74efb6893fc",
      "metadata": {
        "id": "9e7c21a9-05b7-4dc6-818c-e74efb6893fc"
      },
      "source": [
        "### Why use a specific Tokenizer?\n",
        "REF: https://huggingface.co/docs/transformers/tokenizer_summary  \n",
        "Depending on the rules we apply for tokenizing a text, a different tokenized output is generated for the same text. A pretrained model only performs properly if you feed it an input that was tokenized with the same rules that were used to tokenize its training data.  \n",
        "\n",
        "In general, transformers models rarely have a vocabulary size greater than 50,000, especially if they are pretrained only on a single language.  \n",
        "\n",
        "Character tokenization is very simple and would greatly reduce memory and time complexity but it makes it much harder for the model to learn meaningful input representations, and often accompanied by a loss of performance.\n",
        "\n",
        "To get the best of both worlds, `transformers models use a hybrid` between word-level and character-level tokenization called subword tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d0ab91e7-caf4-49ed-8d74-d27c51eac795",
      "metadata": {
        "id": "d0ab91e7-caf4-49ed-8d74-d27c51eac795"
      },
      "outputs": [],
      "source": [
        "# corpus file to process - total line count - 974291 ~1mil\n",
        "file_to_process ='/content/oscar.eo.txt'\n",
        "# file_to_process ='oscar.eo.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e5cc564e-85f3-4769-a32a-848116f8d817",
      "metadata": {
        "id": "e5cc564e-85f3-4769-a32a-848116f8d817"
      },
      "outputs": [],
      "source": [
        "# ### Only RUN this cell if you plan to decrease size of file\n",
        "# ### - decreased since Colab is crashing on this dataset\n",
        "!head -10000 /content/oscar.eo.txt > /content/oscar.eo_small.txt\n",
        "file_to_process ='/content/oscar.eo_small.txt'\n",
        "\n",
        "# local machine\n",
        "# !head -1000 oscar.eo.txt > oscar.eo_small.txt\n",
        "# file_to_process ='oscar.eo_small.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "60e370fd-ed8d-497d-8cc3-714f970de1a2",
      "metadata": {
        "id": "60e370fd-ed8d-497d-8cc3-714f970de1a2"
      },
      "outputs": [],
      "source": [
        "## preprocess text\n",
        "# Represents a Byte-level BPE as introduced by OpenAI with their GPT-2 model\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f91d61a1-9c72-4f64-a1b4-2c9d413e4b63",
      "metadata": {
        "id": "f91d61a1-9c72-4f64-a1b4-2c9d413e4b63"
      },
      "source": [
        "generaly special_tokens list is  \"&lt;s&gt;  &lt;pad&gt; &lt;unk&gt;   &lt;/s&gt;   &lt;mask&gt;\"  \n",
        "since we are training from scratch, special tokens is [&nbsp;]  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "migA7V5IFDmo",
      "metadata": {
        "id": "migA7V5IFDmo"
      },
      "source": [
        "### Train Tokenizer\n",
        "REF: https://discuss.huggingface.co/t/tokenizer-progress-bar/1147/2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "11154ebd-89c6-44ba-a357-26390e40df1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11154ebd-89c6-44ba-a357-26390e40df1d",
        "outputId": "293f4e8e-e647-4c61-f82a-eeee09c52dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing: 100%|██████████| 27/27 [01:19<00:00,  2.96s/text]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "special_tokens = [\"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "\n",
        "def tokenizer_with_progress(large_batch):\n",
        "    for text in tqdm(large_batch, desc=\"Tokenizing\", unit=\"text\"):\n",
        "        tokenizer.train(files=large_batch, vocab_size=52_000, min_frequency=2, special_tokens=special_tokens)\n",
        "\n",
        "tokenizer_with_progress(file_to_process)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6d56e6e4-bd95-4ff4-8c33-8726ee4c2ea0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d56e6e4-bd95-4ff4-8c33-8726ee4c2ea0",
        "outputId": "9cb4ef83-7ecb-41c5-9dcd-4daf92f276ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/tokenizer/RobertaMLM/oscar_eo/vocab.json',\n",
              " '/content/tokenizer/RobertaMLM/oscar_eo/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "## save and check output\n",
        "# did not work - tokenizer(text, vocab_size, min_frequency, special_tokens).to(device)\n",
        "# this might work - encoding = tokenizer(text, return_tensors=\"pt\").to(device)  # REF: https://github.com/huggingface/transformers/issues/16359\n",
        "tokenizer.save_model(tokenizer_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "GZlFI7y7L6kU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZlFI7y7L6kU",
        "outputId": "60827dec-d2fb-46fe-c855-fcf0fb83036d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tokenizer/RobertaMLM/oscar_eo/vocab.json \n",
            " /content/tokenizer/RobertaMLM/oscar_eo/merges.txt\n"
          ]
        }
      ],
      "source": [
        "vocab_file = os.path.join(tokenizer_dir,\"vocab.json\")\n",
        "merges_file = os.path.join(tokenizer_dir,\"merges.txt\")\n",
        "print(vocab_file,'\\n',merges_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4cbcb1c-e94b-46b8-96e7-ae2b5455ea3c",
      "metadata": {
        "id": "a4cbcb1c-e94b-46b8-96e7-ae2b5455ea3c"
      },
      "source": [
        "The tokenizer is optimized for Esperanto - native words are represented by unsplit tokens.  \n",
        "To use in tokenizers combine the generated `vocab.json` and `merges.txt` files to create BBPE tokenizer and post-process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e460f1d4-0d5e-470f-8321-cbaaa33501e8",
      "metadata": {
        "id": "e460f1d4-0d5e-470f-8321-cbaaa33501e8"
      },
      "outputs": [],
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "  vocab_file,\n",
        "  merges_file,\n",
        ")\n",
        "\n",
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "  (\"\", tokenizer.token_to_id(\"\")),\n",
        "  (\"\", tokenizer.token_to_id(\"\")),\n",
        ")\n",
        "\n",
        "tokenizer.enable_truncation(max_length=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b84d8bd-f2b3-4d12-ad93-f77475081a69",
      "metadata": {
        "id": "4b84d8bd-f2b3-4d12-ad93-f77475081a69"
      },
      "source": [
        "test the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "edffba68-d609-49dc-97d6-a34eb670aaaf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edffba68-d609-49dc-97d6-a34eb670aaaf",
        "outputId": "d4c912be-f3ab-49c1-d179-3fc50b4bfd0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding(num_tokens=20, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "['', 'li', 'Ġestis', ':', 'Kar', 'ÅĿena', ',', 'ĠÅľetar', ',', 'ĠAdmata', ',', 'ĠTarÅĿiÅĿ', ',', 'ĠMeres', ',', 'ĠMarsena', ',', 'Ġkaj', 'ĠMemuÄ¥an', '']\n"
          ]
        }
      ],
      "source": [
        "text_line = \"li estis:Karŝena, Ŝetar, Admata, Tarŝiŝ, Meres, Marsena, kaj Memuĥan\"\n",
        "print(tokenizer.encode(text_line))\n",
        "print(tokenizer.encode(text_line).tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d41f53ac-98dd-4bd1-9bc2-1c5c49b43856",
      "metadata": {
        "id": "d41f53ac-98dd-4bd1-9bc2-1c5c49b43856"
      },
      "source": [
        "### Train a RoBERTa-like (BERT) model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "865d3fb5-4a34-425b-9e2c-6054716096f3",
      "metadata": {
        "id": "865d3fb5-4a34-425b-9e2c-6054716096f3"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 16    # input batch size for training (check current default)\n",
        "eval_batch_size = 8      # input batch size for testing (check current default)\n",
        "epochs = 10               # number of epochs to train (check current default)\n",
        "learning_rate = 1e-4     # learning rate (check current default)\n",
        "weight_decay = 0.01\n",
        "maxlength = 128\n",
        "save_steps = 4096\n",
        "save_total_limit = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97af6c6e-355b-4361-ad66-66a5b56561f6",
      "metadata": {
        "id": "97af6c6e-355b-4361-ad66-66a5b56561f6"
      },
      "source": [
        "Setup configuration for ROBERTa to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e78bf4cd-7799-46ae-91fb-a138db46da1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e78bf4cd-7799-46ae-91fb-a138db46da1f",
        "outputId": "61aed2c1-87d3-4231-f460-cb906375e5b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'RobertaTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'RobertaTokenizerFast'.\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_dir, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "810529bf-7157-45af-b8b8-54f093094d3f",
      "metadata": {
        "id": "810529bf-7157-45af-b8b8-54f093094d3f"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaConfig\n",
        "config = RobertaConfig(\n",
        "  vocab_size=52_000,\n",
        "  max_position_embeddings=514,\n",
        "  num_attention_heads=12,\n",
        "  num_hidden_layers=6,\n",
        "  type_vocab_size=1,\n",
        ")\n",
        "\n",
        "# initialize from config (above cell)\n",
        "from transformers import RobertaForMaskedLM\n",
        "model = RobertaForMaskedLM(config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f096a4-c116-4d83-a8c1-48679ae31138",
      "metadata": {
        "id": "e9f096a4-c116-4d83-a8c1-48679ae31138"
      },
      "source": [
        "create tokenizer, and intiate model\n",
        "\n",
        "since we are setting up training from scratch  \n",
        "initialize from config (cell above) and not from a pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ec7b77-ccf2-4584-af7f-322f62936766",
      "metadata": {
        "id": "f4ec7b77-ccf2-4584-af7f-322f62936766"
      },
      "source": [
        "### Build training set\n",
        "create a `Custom` Dataset class to read the `text` file line-by-line  \n",
        "- since this is `Esperanto` language file use encoding=\"utf8\"\n",
        "- maxlength = 128\n",
        "- create a `examples` dataset key to store tokenized `input_ids`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "HyHl94mxe8uT",
      "metadata": {
        "id": "HyHl94mxe8uT"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "      self.examples = []\n",
        "      with open(df, 'r', encoding=\"utf8\") as f:\n",
        "        lines = f.readlines()\n",
        "        num_lines = len(lines)\n",
        "        # for i, line in enumerate(tqdm(f))\n",
        "        for example in tqdm(lines, total=num_lines):\n",
        "          # for example in lines:\n",
        "          x=tokenizer.encode_plus(example, max_length = maxlength, truncation=True, padding=True)\n",
        "          self.examples += [x.input_ids]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # We’ll pad at the batch level.\n",
        "        return torch.tensor(self.examples[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3851031-c4d4-40b5-bb75-3e94fbe35d4f",
      "metadata": {
        "id": "f3851031-c4d4-40b5-bb75-3e94fbe35d4f"
      },
      "source": [
        "`tokenize` Esperanto text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4a20b2b5-723a-4cae-bef4-abff75aad6b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a20b2b5-723a-4cae-bef4-abff75aad6b2",
        "outputId": "31495dae-d25d-43f5-85fe-bac8f3f04d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:03<00:00, 3200.09it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = CustomDataset(file_to_process, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "562e1cea-b9b5-43fb-a95a-7099d6ba24e8",
      "metadata": {
        "id": "562e1cea-b9b5-43fb-a95a-7099d6ba24e8"
      },
      "source": [
        "### create a `DataCollator` for `LanguageModeling`  \n",
        "REF: https://huggingface.co/docs/transformers/main_classes/data_collator  \n",
        "\n",
        "- Data collators are objects that will form a batch by using a list of dataset elements as input.\n",
        "- Apply some random data augmentation (like random masking) on the formed batch (`Language Modeling`)\n",
        "- [Example Scripts](https://huggingface.co/docs/transformers/examples)\n",
        "- [Example Notebooks](https://huggingface.co/docs/transformers/notebooks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a9703283-2949-43dd-b03a-2895df48440e",
      "metadata": {
        "id": "a9703283-2949-43dd-b03a-2895df48440e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6fa7a21-aca4-4c48-8e56-8d24b63d5647"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "# from transformers import DataCollatorWithPadding\n",
        "\n",
        "# Define the Data Collator - use Masking with 15% probability of <mask> usage\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db78f452-d08d-48b0-994c-fe233b0fb51b",
      "metadata": {
        "id": "db78f452-d08d-48b0-994c-fe233b0fb51b"
      },
      "source": [
        "### split dataset -to- train and test sets - use `random_split`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2d0b6515-a0a1-4348-b931-716cda830139",
      "metadata": {
        "id": "2d0b6515-a0a1-4348-b931-716cda830139"
      },
      "outputs": [],
      "source": [
        "## split into train and test sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "eval_size = int(0.5 * len(test_dataset))\n",
        "eval_dataset, test_dataset = torch.utils.data.random_split(test_dataset, [eval_size, eval_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "73a23a5e-099c-4d03-ac4e-176bc2324f0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73a23a5e-099c-4d03-ac4e-176bc2324f0f",
        "outputId": "04c32ff9-2d9c-4e5b-c55c-3b4e056b4e37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24,\n",
              " [46038,\n",
              "  2462,\n",
              "  2274,\n",
              "  12522,\n",
              "  1004,\n",
              "  13156,\n",
              "  1004,\n",
              "  2508,\n",
              "  12627,\n",
              "  5712,\n",
              "  10405,\n",
              "  1004,\n",
              "  9365,\n",
              "  5712,\n",
              "  8523,\n",
              "  1004,\n",
              "  2166,\n",
              "  1004,\n",
              "  5880,\n",
              "  5712,\n",
              "  4978,\n",
              "  12844,\n",
              "  199,\n",
              "  46039])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "## check one to verify correctness\n",
        "sample = train_dataset.dataset.examples[0]\n",
        "len(sample), sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b74972c5-721c-4b34-9999-dd707ae4d903",
      "metadata": {
        "id": "b74972c5-721c-4b34-9999-dd707ae4d903"
      },
      "source": [
        "### Visualize Model Summary and Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "df5f2f21-c42e-47e6-ac97-76ee9aacf664",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df5f2f21-c42e-47e6-ac97-76ee9aacf664",
        "outputId": "853437f7-0fbf-4279-93dd-199d47a6a7b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 514,\n",
              "  \"model_type\": \"roberta\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.38.2\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 52000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# print config\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f43ac494-0130-4242-a479-42865fd605e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f43ac494-0130-4242-a479-42865fd605e7",
        "outputId": "c46bf024-6f79-42ae-8a19-11017484de12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83504416"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "## number of parameters\n",
        "model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "957d6a53-dc63-4d30-ac40-8eda28ff0605",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "957d6a53-dc63-4d30-ac40-8eda28ff0605",
        "outputId": "5ac6e67e-39d5-4ea4-bab0-f0fe01b3ca7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=====================================================================================\n",
              "Layer (type:depth-idx)                                       Param #\n",
              "=====================================================================================\n",
              "RobertaForMaskedLM                                           --\n",
              "├─RobertaModel: 1-1                                          --\n",
              "│    └─RobertaEmbeddings: 2-1                                --\n",
              "│    │    └─Embedding: 3-1                                   39,936,000\n",
              "│    │    └─Embedding: 3-2                                   394,752\n",
              "│    │    └─Embedding: 3-3                                   768\n",
              "│    │    └─LayerNorm: 3-4                                   1,536\n",
              "│    │    └─Dropout: 3-5                                     --\n",
              "│    └─RobertaEncoder: 2-2                                   --\n",
              "│    │    └─ModuleList: 3-6                                  42,527,232\n",
              "├─RobertaLMHead: 1-2                                         --\n",
              "│    └─Linear: 2-3                                           590,592\n",
              "│    └─LayerNorm: 2-4                                        1,536\n",
              "│    └─Linear: 2-5                                           39,988,000\n",
              "=====================================================================================\n",
              "Total params: 123,440,416\n",
              "Trainable params: 123,440,416\n",
              "Non-trainable params: 0\n",
              "====================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "gc.collect()\n",
        "\n",
        "from torchinfo import summary\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6696b3f-9010-4561-a7c8-809317738290",
      "metadata": {
        "id": "c6696b3f-9010-4561-a7c8-809317738290"
      },
      "source": [
        "### Dataloaders\n",
        "REF: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html?highlight=dataloader\n",
        "DataLoader and Dataset allow you to use pre-loaded datasets as well as your own data  \n",
        "\n",
        "- Dataset stores the samples and their corresponding labels\n",
        "- DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "770deed3-6861-4661-8a5a-6a0d6cf206a5",
      "metadata": {
        "id": "770deed3-6861-4661-8a5a-6a0d6cf206a5"
      },
      "outputs": [],
      "source": [
        "test_dataloader = DataLoader(\n",
        "    test_dataset.dataset.dataset.examples,\n",
        "    batch_size=eval_batch_size,\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset.dataset.dataset.examples,\n",
        "    batch_size=eval_batch_size,\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "KZ3rNAfs20R0",
      "metadata": {
        "id": "KZ3rNAfs20R0"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    train_dataset.dataset.examples,\n",
        "    batch_size=train_batch_size,\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88a599a4-57bd-4079-b917-1799f510f6e5",
      "metadata": {
        "id": "88a599a4-57bd-4079-b917-1799f510f6e5"
      },
      "source": [
        "### Define TrainingArguments to Trainer\n",
        "Customize how the `model` is going to be trained.  \n",
        "- hyperparameters you can tune - example:\n",
        "  - `learning_rate`, `weight_decay`, `batch_size` (train/eval), `epochs`\n",
        "- flags for activating different training options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "c8666a64-e607-4980-8a1d-c8186f9c03ac",
      "metadata": {
        "id": "c8666a64-e607-4980-8a1d-c8186f9c03ac"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_folder,\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    num_train_epochs=epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    per_device_train_batch_size=train_batch_size, # use these instead (preferred)\n",
        "    per_device_eval_batch_size=eval_batch_size,\n",
        "    save_steps=save_steps,\n",
        "    # eval_steps=eval_steps,\n",
        "    # remove_unused_columns=False,\n",
        "    save_total_limit=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "252df890-2e85-4f09-9300-acb523967e21",
      "metadata": {
        "id": "252df890-2e85-4f09-9300-acb523967e21"
      },
      "source": [
        "### Create Trainer for the model  \n",
        "REF: [Huggingface](https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/trainer.md)\n",
        "[Google Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)\n",
        "Supports `distributed training` on multiple GPUs/TPUs  \n",
        "Goes hand-in-hand with `TrainingArguments` class  \n",
        "`Trainer` class is optimized for `Huggingface` Transformer models makes it easier to start training instead of writing your own training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbba5d02-eb0c-443c-86f8-5c5f57884b26",
      "metadata": {
        "id": "bbba5d02-eb0c-443c-86f8-5c5f57884b26"
      },
      "source": [
        "### Evaluation Metric\n",
        "REF: [Google Colab Training](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb#scrollTo=I7_tzailp4SP)  \n",
        "\n",
        "Trainer does not automatically evaluate model performance. pass a function to compute and report metrics  \n",
        "Before passing your predictions to compute, convert predictions to logits - `Transformer` models return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "220b9649-1457-4f84-8289-3fe396451e01",
      "metadata": {
        "id": "220b9649-1457-4f84-8289-3fe396451e01"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import evaluate\n",
        "# metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "# ## compute_metrics function\n",
        "# def compute_metrics(eval_pred):\n",
        "#   logits, labels = eval_pred\n",
        "#   predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "#   return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f210d6c5-87a3-4b1c-ab3c-e513852b604f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f210d6c5-87a3-4b1c-ab3c-e513852b604f",
        "outputId": "f982a148-c1c3-4373-d510-cd673ea88a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataloader.dataset,\n",
        "    eval_dataset=eval_dataloader.dataset,\n",
        "    ## train_dataset=train_dataset.dataset.examples,\n",
        "    ## eval_dataset=eval_dataset.dataset.examples,\n",
        "    # compute_metrics=compute_metrics,                 ## This requires a lot of memory\n",
        "    # prediction_loss_only=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "MMVgWSmqgIDr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMVgWSmqgIDr",
        "outputId": "df02406a-7e61-4d63-cbe4-16bd30ae84dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "training_args.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "250e6730-2f87-4737-9ca5-9545cc9be289",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "250e6730-2f87-4737-9ca5-9545cc9be289",
        "outputId": "c7071966-ea50-44e7-fcd3-616505eda653"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6250/6250 22:17, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>8.092800</td>\n",
              "      <td>7.522412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>7.491800</td>\n",
              "      <td>7.171335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>7.267900</td>\n",
              "      <td>6.969954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.985600</td>\n",
              "      <td>6.790073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>6.864500</td>\n",
              "      <td>6.684762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>6.755800</td>\n",
              "      <td>6.573242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>6.691500</td>\n",
              "      <td>6.478450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>6.560700</td>\n",
              "      <td>6.385944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>6.543700</td>\n",
              "      <td>6.366913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>6.497100</td>\n",
              "      <td>6.355762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6250, training_loss=6.93523205078125, metrics={'train_runtime': 1338.8347, 'train_samples_per_second': 74.692, 'train_steps_per_second': 4.668, 'total_flos': 3201789003918336.0, 'train_loss': 6.93523205078125, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1efe4335-71ce-4cc0-816d-21254ec373f2",
      "metadata": {
        "id": "1efe4335-71ce-4cc0-816d-21254ec373f2"
      },
      "source": [
        "### Perplexity\n",
        "REF: https://huggingface.co/docs/transformers/perplexity   \n",
        "Perplexity (PPL) is one of the most common metrics for evaluating language models.   \n",
        "\n",
        "This metric applies specifically to classical language models (sometimes called autoregressive or causal language models) and is not well defined for masked language models like BERT  \n",
        "\n",
        "Perplexity is defined as the exponentiated average negative log-likelihood of a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "7d03c576-0e56-4305-b676-c6a02e23e189",
      "metadata": {
        "id": "7d03c576-0e56-4305-b676-c6a02e23e189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "afe6d831-e19c-4a5c-afad-874e43560143"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 00:30]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 575.65\n",
            "{'eval_loss': 6.355495929718018, 'eval_runtime': 30.7466, 'eval_samples_per_second': 325.239, 'eval_steps_per_second': 40.655, 'epoch': 10.0}\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
        "print(eval_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "737d60d7-c85b-4753-8298-0bfeef54363b",
      "metadata": {
        "id": "737d60d7-c85b-4753-8298-0bfeef54363b"
      },
      "source": [
        "The run is currently for `10` epoch. If you train for more epochs, the loss will decrease. Also we are training on only 10k lines of the available data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18981d11-a8e4-4fe1-9d39-8acab1f4beee",
      "metadata": {
        "id": "18981d11-a8e4-4fe1-9d39-8acab1f4beee"
      },
      "source": [
        "### save model and tokenizer to disk  \n",
        "save model and tokenizer to disk for future tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "92d9a5c4-da15-4fb5-af1e-51bca5329ffa",
      "metadata": {
        "id": "92d9a5c4-da15-4fb5-af1e-51bca5329ffa"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(model_folder)\n",
        "configuration = model.config             # not saving to model_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "d2fcec39-e471-44ea-94a0-ccf14328f200",
      "metadata": {
        "id": "d2fcec39-e471-44ea-94a0-ccf14328f200"
      },
      "outputs": [],
      "source": [
        "# save config.json to both model_folder and tokenizer_dir\n",
        "model.save_pretrained(model_folder)\n",
        "model.save_pretrained(tokenizer_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241df69f-5be0-452a-86e6-69bf5afe47c1",
      "metadata": {
        "id": "241df69f-5be0-452a-86e6-69bf5afe47c1"
      },
      "source": [
        "### Checking the trained model using a Pipeline\n",
        "Looking at the training and eval losses going down is not enough, we would like to apply our model to check if our language model is learning anything interesting. An easy way is via the FillMaskPipeline.\n",
        "\n",
        "Pipelines are simple wrappers around tokenizers and models. We can use the `fill-mask` pipeline where we input a sequence containing a masked token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "3f57bbc1-a1f6-40f8-9bed-4a6a47cbfe6b",
      "metadata": {
        "id": "3f57bbc1-a1f6-40f8-9bed-4a6a47cbfe6b"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "from transformers import pipeline\n",
        "# Create a Fill mask pipeline\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=model_folder,\n",
        "    tokenizer=tokenizer_dir,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "25956bac-2155-4c7e-955e-9032b12321b9",
      "metadata": {
        "id": "25956bac-2155-4c7e-955e-9032b12321b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c62281-3c7d-4c8c-f147-302f0b53b4f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'score': 0.08599499613046646,\n",
              "   'token': 259,\n",
              "   'token_str': ' la',\n",
              "   'sequence': '<s>Lerni Esperanton per telefono, novaĵoj la 120 jaroj de<mask> Svitavy-Polička</s>'},\n",
              "  {'score': 0.0667266994714737,\n",
              "   'token': 13,\n",
              "   'token_str': '-',\n",
              "   'sequence': '<s>Lerni Esperanton per telefono, novaĵoj- 120 jaroj de<mask> Svitavy-Polička</s>'},\n",
              "  {'score': 0.053006891161203384,\n",
              "   'token': 12,\n",
              "   'token_str': ',',\n",
              "   'sequence': '<s>Lerni Esperanton per telefono, novaĵoj, 120 jaroj de<mask> Svitavy-Polička</s>'},\n",
              "  {'score': 0.03821343928575516,\n",
              "   'token': 285,\n",
              "   'token_str': ' kaj',\n",
              "   'sequence': '<s>Lerni Esperanton per telefono, novaĵoj kaj 120 jaroj de<mask> Svitavy-Polička</s>'},\n",
              "  {'score': 0.025875721126794815,\n",
              "   'token': 270,\n",
              "   'token_str': ' de',\n",
              "   'sequence': '<s>Lerni Esperanton per telefono, novaĵoj de 120 jaroj de<mask> Svitavy-Polička</s>'}],\n",
              " [{'score': 0.09585653990507126,\n",
              "   'token': 259,\n",
              "   'token_str': ' la',\n",
              "   'sequence': '<s>Lerni Esperanton per telefono, novaĵoj<mask> 120 jaroj de la Svitavy-Polička</s>'},\n",
              "  {'score': 0.05452503263950348,\n",
              "   'token': 13,\n",
              "   'token_str': '-',\n",
              "   'sequence': '<s>Lerni Esperanton per telefono, novaĵoj<mask> 120 jaroj de- Svitavy-Polička</s>'},\n",
              "  {'score': 0.04598027840256691,\n",
              "   'token': 12,\n",
              "   'token_str': ',',\n",
              "   'sequence': '<s>Lerni Esperanton per telefono, novaĵoj<mask> 120 jaroj de, Svitavy-Polička</s>'},\n",
              "  {'score': 0.03660845384001732,\n",
              "   'token': 285,\n",
              "   'token_str': ' kaj',\n",
              "   'sequence': '<s>Lerni Esperanton per telefono, novaĵoj<mask> 120 jaroj de kaj Svitavy-Polička</s>'},\n",
              "  {'score': 0.02784859761595726,\n",
              "   'token': 270,\n",
              "   'token_str': ' de',\n",
              "   'sequence': '<s>Lerni Esperanton per telefono, novaĵoj<mask> 120 jaroj de de Svitavy-Polička</s>'}]]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "### Inference using Test Set\n",
        "# Test some examples\n",
        "## The test text: \"Lerni Esperanton per telefono, novaĵoj Poŝtkarto 120 jaroj de fervojo Svitavy-Polička\"\n",
        "fill_mask(\"Lerni Esperanton per telefono, novaĵoj <mask> 120 jaroj de <mask> Svitavy-Polička\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "42d00678-c91f-4449-9ab4-a2a3ff645e02",
      "metadata": {
        "id": "42d00678-c91f-4449-9ab4-a2a3ff645e02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5826ada-7f1e-435f-b6fb-99b022f767d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'score': 0.973590075969696,\n",
              "   'token': 1055,\n",
              "   'token_str': ' disponeblas',\n",
              "   'sequence': '<s>La teksto disponeblas laŭ la permesilo Krea<mask> Atribuite-Samkondiĉe</s>'},\n",
              "  {'score': 0.0020416462793946266,\n",
              "   'token': 1068,\n",
              "   'token_str': ' Neadaptita',\n",
              "   'sequence': '<s>La teksto Neadaptita laŭ la permesilo Krea<mask> Atribuite-Samkondiĉe</s>'},\n",
              "  {'score': 0.0009146895026788116,\n",
              "   'token': 1432,\n",
              "   'token_str': ' sistemo',\n",
              "   'sequence': '<s>La teksto sistemo laŭ la permesilo Krea<mask> Atribuite-Samkondiĉe</s>'},\n",
              "  {'score': 0.0008044862770475447,\n",
              "   'token': 984,\n",
              "   'token_str': ' Komunaĵo',\n",
              "   'sequence': '<s>La teksto Komunaĵo laŭ la permesilo Krea<mask> Atribuite-Samkondiĉe</s>'},\n",
              "  {'score': 0.0007535951444879174,\n",
              "   'token': 1965,\n",
              "   'token_str': ' informojn',\n",
              "   'sequence': '<s>La teksto informojn laŭ la permesilo Krea<mask> Atribuite-Samkondiĉe</s>'}],\n",
              " [{'score': 0.9900994896888733,\n",
              "   'token': 984,\n",
              "   'token_str': ' Komunaĵo',\n",
              "   'sequence': '<s>La teksto<mask> laŭ la permesilo Krea Komunaĵo Atribuite-Samkondiĉe</s>'},\n",
              "  {'score': 0.001063838484697044,\n",
              "   'token': 1068,\n",
              "   'token_str': ' Neadaptita',\n",
              "   'sequence': '<s>La teksto<mask> laŭ la permesilo Krea Neadaptita Atribuite-Samkondiĉe</s>'},\n",
              "  {'score': 0.0005927464226260781,\n",
              "   'token': 1878,\n",
              "   'token_str': ' 2000',\n",
              "   'sequence': '<s>La teksto<mask> laŭ la permesilo Krea 2000 Atribuite-Samkondiĉe</s>'},\n",
              "  {'score': 0.0005041410331614316,\n",
              "   'token': 924,\n",
              "   'token_str': ' permesilo',\n",
              "   'sequence': '<s>La teksto<mask> laŭ la permesilo Krea permesilo Atribuite-Samkondiĉe</s>'},\n",
              "  {'score': 0.0004366773646324873,\n",
              "   'token': 937,\n",
              "   'token_str': ' kondiĉoj',\n",
              "   'sequence': '<s>La teksto<mask> laŭ la permesilo Krea kondiĉoj Atribuite-Samkondiĉe</s>'}]]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "## The test text: \"La teksto disponeblas laŭ la permesilo Krea Komunaĵo Atribuite-Samkondiĉe\"\n",
        "fill_mask(\"La teksto <mask> laŭ la permesilo Krea <mask> Atribuite-Samkondiĉe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "132ad0e6-bc05-4199-b46a-cc549be3ac59",
      "metadata": {
        "id": "132ad0e6-bc05-4199-b46a-cc549be3ac59"
      },
      "source": [
        "### check scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ca4ab10-6c90-4255-bf00-d9df02e015bb",
      "metadata": {
        "id": "3ca4ab10-6c90-4255-bf00-d9df02e015bb"
      },
      "source": [
        "### ROUGE – Recall-Oriented Understudy for Gisting Evaluation\n",
        "REF: https://mlexplained.blog/2023/07/08/large-language-model-llm-evaluation-metrics-bleu-and-rouge/  \n",
        "`Usually used for SUMMARIZATION` tasks  \n",
        "\n",
        "Evaluation metric for assessing the quality of automatic summaries generated by text summarization systems. It measures the similarity between the generated summary and one or more reference summaries.  \n",
        "\n",
        "Calculates the precision and recall scores by comparing the n-gram units (such as words or sequences of words) in the generated summary with those in the reference summaries. It focuses on the recall score, which measures how much of the important information from the reference summaries is captured by the generated summary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7daeedad-4156-44b6-a0c7-c6583b8fcf85",
      "metadata": {
        "id": "7daeedad-4156-44b6-a0c7-c6583b8fcf85"
      },
      "source": [
        "### BLEU – Bilingual Evaluation Understudy\n",
        "REF: https://mlexplained.blog/2023/07/08/large-language-model-llm-evaluation-metrics-bleu-and-rouge/  \n",
        "https://huggingface.co/spaces/evaluate-metric/bleu  \n",
        "`Usually used for MACHINE TRANSLATION` tasks  \n",
        "\n",
        "Evaluates the quality of machine-generated translations against one or more reference translations. It measures the similarity between the machine-generated translation and the reference translations based on the n-grams.  \n",
        "\n",
        "BLEU score ranges from 0 to 1, with a higher score indicating a better match between the generated translation and the references. A score of 1 means a perfect match, while a score of 0 means no overlap between the generated and reference translations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2abea58-4a91-400e-bfab-5ce85c5a24d3",
      "metadata": {
        "id": "b2abea58-4a91-400e-bfab-5ce85c5a24d3"
      },
      "source": [
        "### BERT Score\n",
        "REF: https://huggingface.co/spaces/evaluate-metric/bertscore  \n",
        "`computes a similarity score for each token in the candidate sentence with each token in the reference sentence`  \n",
        "\n",
        "BERTScore leverages the pre-trained contextual embeddings from BERT and matches words in candidate and reference sentences by cosine similarity. It has been shown to correlate with human judgment on sentence-level and system-level evaluation. Moreover, BERTScore computes precision, recall, and F1 measure, which can be useful for evaluating different language generation tasks.\n",
        "\n",
        "BERTScore is an automatic evaluation metric which leverages the pre-trained contextual embeddings from [BERT](https://huggingface.co/bert-base-uncased) models and matches words in candidate and reference sentences by cosine similarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8508aec3-1c8f-4501-a9cc-11f64d6ff7ad",
      "metadata": {
        "id": "8508aec3-1c8f-4501-a9cc-11f64d6ff7ad"
      },
      "source": [
        "### References:\n",
        "**Huggingface Blog** [How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train)   \n",
        "**Huggingface official documentation** [Encoder-Decoder models](https://huggingface.co/transformers/model_doc/encoderdecoder.html)  \n",
        "**Huggingface Model** [RoBERTa documentation](https://huggingface.co/transformers/model_doc/roberta.html)  \n",
        "**Tokenizer** [Huggingface Tokenizer Documentation](https://huggingface.co/transformers/tokenizer_summary.html)  \n",
        "**Wikipedia definition** [Byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)  \n",
        "**Medium Blog** [Create a Tokenizer and train a Huggingface Model](https://medium.com/analytics-vidhya/create-a-tokenizer-and-train-a-huggingface-roberta-model-from-scratch-f3ed1138180c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "176f2bc2-4994-410b-b9b4-396f3152d72d",
      "metadata": {
        "id": "176f2bc2-4994-410b-b9b4-396f3152d72d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcddb810-a923-4c54-8f91-c6e93545e4d6",
      "metadata": {
        "id": "dcddb810-a923-4c54-8f91-c6e93545e4d6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
